{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install ../\n",
    "# ! pip install matplotlib\n",
    "# ! pip install langchain\n",
    "# ! pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functionality presented below was first implemented as a Chain in langchain and can be found [here](https://github.com/langchain-ai/langchain/blob/master/cookbook/learned_prompt_optimization.ipynb)\n",
    "\n",
    "LLM prompts can be enhanced by injecting specific terms into template sentences. Selecting the right terms is crucial for obtaining high-quality responses. This notebook introduces automated prompt engineering through term injection using `learn_to_pick.PickBest`.\n",
    "\n",
    "For illustration, consider the scenario of a meal delivery service. We want to ask customers, like Tom, about their dietary preferences and recommend suitable meals from our extensive menu. The picker selects a meal based on user preferences, injects it into a prompt template, and forwards the prompt to an LLM. The LLM's response, which is a personalized recommendation, is then returned to the user.\n",
    "\n",
    "The example laid out below is a toy example to demonstrate the applicability of the concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# four meals defined, some vegetarian some not\n",
    "\n",
    "meals = [\n",
    "    \"Beef Enchiladas with Feta cheese. Mexican-Greek fusion\",\n",
    "    \"Chicken Flatbreads with red sauce. Italian-Mexican fusion\",\n",
    "    \"Veggie sweet potato quesadillas with vegan cheese\",\n",
    "    \"One-Pan Tortelonni bake with peppers and onions\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick and configure the LLM of your choice\n",
    "\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=\"gpt-4\",\n",
    "    temperature=0,\n",
    "    request_timeout=10,\n",
    "    max_retries=3,\n",
    "    client=None,\n",
    ")\n",
    "\n",
    "llm.predict(\"hey\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lets define our prompt that will be personalized\n",
    "\n",
    "The prompt template which will be used to personalize the message using the LLM call needs to be defined.\n",
    "It can be anything, but here `{meal}` is being used and is going to be replaced by one of the meals above, the picker will try to pick and inject the best meal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# here I am using the variable meal which will be replaced by one of the meals above\n",
    "# and some variables like user, preference, and text_to_personalize which I will provide at chain run time\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"Here is the description of a meal: \"{meal}\".\n",
    "\n",
    "Embed the meal into the given text: \"{text_to_personalize}\".\n",
    "\n",
    "Prepend a personalized message including the user's name \"{user}\" \n",
    "    and their preference \"{preference}\".\n",
    "\n",
    "Make it sound good. Do NOT change the description of the meal.\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"meal\", \"text_to_personalize\", \"user\", \"preference\"], \n",
    "    template=PROMPT_TEMPLATE\n",
    ")\n",
    "\n",
    "# create the llm chain and initialize it with the prompt\n",
    "from langchain import  LLMChain\n",
    "llm_for_text_generation = LLMChain(prompt=PROMPT, llm=llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import learn_to_pick\n",
    "\n",
    "# the llm provided here is going to serve as the scoring llm\n",
    "picker = learn_to_pick.PickBest.create(llm=llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = picker.run(\n",
    "    meal = learn_to_pick.ToSelectFrom(meals),\n",
    "    user = learn_to_pick.BasedOn(\"Tom\"),\n",
    "    preference = learn_to_pick.BasedOn([\"Vegetarian\", \"regular dairy is ok\"]),\n",
    ")\n",
    "\n",
    "picked_meal = response[\"picked\"][0][\"meal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beef Enchiladas with Feta cheese. Mexican-Greek fusion\n"
     ]
    }
   ],
   "source": [
    "print(picked_meal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hey Tom! We know you're a Vegetarian, but we think you might be tempted by this week's specialty dish. Our master chefs have created a delicious Mexican-Greek fusion: Beef Enchiladas with Feta cheese. They believe you will absolutely love it!\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_for_text_generation.run(meal = picked_meal, user = \"Tom\", preference = \"Vegetarian\", text_to_personalize = \"This is the weeks specialty dish, our master chefs believe you will love it!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative example\n",
    "\n",
    "Below is an alternative way to achieve the same result.\n",
    "\n",
    "Additionally it showcases how the llm response can be used in the scorer to determine the quality of the decision.\n",
    "\n",
    "The picker will:\n",
    "- make a selection using the decision making policy\n",
    "- call the scorer to evaluate the decision\n",
    "- update the decision making policy with the score\n",
    "\n",
    "Callback functions can be registered at create time and they will be called after the decision has been made and before the scorer is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of setting own AutoSelectionScorer prompt\n",
    "\n",
    "# the below scoring prompt wants to use the llm_response to determine whether the meal is good or bad\n",
    "\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "import langchain\n",
    "\n",
    "REWARD_PROMPT_TEMPLATE = \"\"\"\n",
    "\n",
    "Given preference: \"{preference}\" rank how good or bad this selection is selection: \"{meal}\" using the full text to see if it fits: \"{llm_response}\"\n",
    "\n",
    "IMPORANT: you MUST return a single number between -1 and 1, -1 being bad, 1 being good\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "REWARD_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"preference\", \"meal\", \"llm_response\"],\n",
    "    template=REWARD_PROMPT_TEMPLATE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the above custom reward prompt wants to use the llm text generated response to determine whether the selection was good or bad, we can register a custom function that will call the `llm_for_text_generation` and set the response in the inputs. The callback will be called before scoring, so the custom scorer will have access to the generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CallbackClass:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "\n",
    "    def set_llm_response(self, inputs, picked, event):\n",
    "        print(f\"from callback function, here are the inputs: {inputs}\")\n",
    "        \n",
    "        response = self.llm.predict(**inputs)\n",
    "        print(f\"response from llm: {response}\")\n",
    "        inputs.update({\"llm_response\": response})\n",
    "        return inputs, picked, event\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from callback function, here are the inputs: {'meal': 'Veggie sweet potato quesadillas with vegan cheese', 'user': Tom, 'preference': ['Vegetarian', 'regular dairy is ok'], 'text_to_personalize': 'This is the weeks specialty dish, our master chefs believe you will love it!', 'rl_chain_selected_based_on': \"{'user': ['Tom'], 'preference': ['Vegetarian', 'regular dairy is ok']}\", 'rl_chain_selected': 'Veggie sweet potato quesadillas with vegan cheese'}\n",
      "response from llm: Hey Tom! As a Vegetarian who's okay with regular dairy, we've got the perfect dish for you this week. Introducing our specialty dish: Veggie sweet potato quesadillas with vegan cheese! Our master chefs believe you will absolutely love this delightful and satisfying meal. Don't miss out on this delicious treat!\n"
     ]
    }
   ],
   "source": [
    "the_callback_class = CallbackClass(llm=llm_for_text_generation)\n",
    "\n",
    "chain = learn_to_pick.PickBest.create(\n",
    "    callbacks_before_scoring = [the_callback_class.set_llm_response],\n",
    "    selection_scorer=learn_to_pick.AutoSelectionScorer(llm=llm, prompt=REWARD_PROMPT),\n",
    ")\n",
    "\n",
    "response = chain.run(\n",
    "    meal = learn_to_pick.ToSelectFrom(meals),\n",
    "    user = learn_to_pick.BasedOn(\"Tom\"),\n",
    "    preference = learn_to_pick.BasedOn([\"Vegetarian\", \"regular dairy is ok\"]),\n",
    "    text_to_personalize = \"This is the weeks specialty dish, our master chefs believe you will love it!\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hey Tom! As a Vegetarian who's okay with regular dairy, we've got the perfect dish for you this week. Introducing our specialty dish: Veggie sweet potato quesadillas with vegan cheese! Our master chefs believe you will absolutely love this delightful and satisfying meal. Don't miss out on this delicious treat!\""
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can now get the llm response from here\n",
    "response[\"picked_metadata\"].outputs[\"llm_response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if we want to see what the score was, that was set from the auto selection scorer\n",
    "response[\"picked_metadata\"].selected.score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
